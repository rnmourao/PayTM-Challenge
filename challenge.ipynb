{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv': virtualenv)",
   "display_name": "Python 3.8.5 64-bit ('venv': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "4389cfd4b26858781fc552a10b8291a92b05c799d1fc59497b8eb379bc7b14c3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, to_timestamp, col, when, sum, avg, count, split, explode, udf, collect_list, monotonically_increasing_id, row_number, size \n",
    "from pyspark.sql.functions import substring, col, to_date\n",
    "from pyspark import Row\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, TimestampType\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "source": [
    "# Step 1 - Setting Up the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Load the global weather data into your big data technology of choice."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = spark.read \\\n",
    "               .format(\"csv\") \\\n",
    "               .option(\"header\", \"true\") \\\n",
    "               .load(\"data/2019/*.csv\")\n",
    "weather.createOrReplaceTempView('weather')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+------+-----+-----+-----+------+\n|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST|   MAX|  MIN| PRCP| SNDP|FRSHTT|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+------+-----+-----+-----+------+\n|958360|99999|20190101|78.8|54.9|9999.9|9999.9|999.9| 8.8| 13.0|999.9| 96.1*| 61.9|0.00G|999.9|000000|\n|958360|99999|20190102|73.1|53.7|9999.9|9999.9|999.9| 9.5| 14.0|999.9| 89.2*|57.4*|0.00G|999.9|000000|\n|958360|99999|20190103|79.5|47.4|9999.9|9999.9|999.9| 3.2|  8.0|999.9| 96.6*| 57.2|0.00G|999.9|000000|\n|958360|99999|20190104|82.7|52.0|9999.9|9999.9|999.9|13.0| 19.0|999.9|109.8*| 60.6|0.02G|999.9|000000|\n|958360|99999|20190105|61.9|47.7|9999.9|9999.9|999.9| 8.5| 15.9|999.9| 70.5*|52.3*|0.02G|999.9|010000|\n|958360|99999|20190106|68.6|48.1|9999.9|9999.9|999.9| 9.2| 13.0|999.9| 79.9*| 52.0|0.00G|999.9|000000|\n|958360|99999|20190107|75.3|53.3|9999.9|9999.9|999.9| 5.9|  9.9|999.9| 87.8*| 55.8|0.00G|999.9|000000|\n|958360|99999|20190108|73.9|47.6|9999.9|9999.9|999.9|10.5| 15.0|999.9| 91.6*|59.0*|0.00G|999.9|000000|\n|958360|99999|20190109|65.9|42.2|9999.9|9999.9|999.9|10.1| 15.0|999.9| 77.2*|53.4*|0.00G|999.9|000000|\n|958360|99999|20190110|68.7|44.0|9999.9|9999.9|999.9| 6.8|  8.0|999.9| 79.2*| 53.1|0.00G|999.9|000000|\n|958360|99999|20190111|78.4|44.9|9999.9|9999.9|999.9| 5.9|  9.9|999.9| 92.7*| 53.8|0.00G|999.9|000000|\n|958360|99999|20190112|77.5|46.2|9999.9|9999.9|999.9|13.1| 19.0|999.9|101.3*|60.8*|0.00G|999.9|000000|\n|958360|99999|20190113|80.8|57.0|9999.9|9999.9|999.9| 7.6| 12.0|999.9| 94.3*| 60.3|0.00G|999.9|000000|\n|958360|99999|20190114|84.7|56.8|9999.9|9999.9|999.9| 4.1|  8.0|999.9|102.7*| 62.8|0.00G|999.9|000000|\n|958360|99999|20190115|87.5|43.7|9999.9|9999.9|999.9| 5.0|  8.0|999.9|106.2*| 64.0|0.00G|999.9|000000|\n|958360|99999|20190116|88.8|52.0|9999.9|9999.9|999.9| 6.9| 21.0|999.9|105.3*| 65.8|0.00G|999.9|000000|\n|958360|99999|20190117|88.3|55.2|9999.9|9999.9|999.9| 8.6| 12.0|999.9| 99.0*| 76.5|0.00G|999.9|000000|\n|958360|99999|20190118|78.0|55.6|9999.9|9999.9|999.9|10.0| 18.1|999.9| 97.7*|60.6*|0.00G|999.9|000000|\n|958360|99999|20190119|71.7|45.0|9999.9|9999.9|999.9|11.5| 15.0|999.9| 82.9*| 59.4|0.00G|999.9|000000|\n|958360|99999|20190120|77.7|56.0|9999.9|9999.9|999.9| 8.4| 14.0|999.9| 93.2*| 60.8|0.00G|999.9|000000|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+------+-----+-----+-----+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = spark.read \\\n",
    "                 .format(\"csv\") \\\n",
    "                 .option(\"header\", \"true\") \\\n",
    "                 .load(\"countrylist.csv\")\n",
    "countries.createOrReplaceTempView('countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+--------------------+\n|COUNTRY_ABBR|        COUNTRY_FULL|\n+------------+--------------------+\n|          AA|               ARUBA|\n|          AC| ANTIGUA AND BARBUDA|\n|          AF|         AFGHANISTAN|\n|          AG|             ALGERIA|\n|          AI|    ASCENSION ISLAND|\n|          AJ|          AZERBAIJAN|\n|          AL|             ALBANIA|\n|          AM|             ARMENIA|\n|          AN|             ANDORRA|\n|          AO|              ANGOLA|\n|          AQ|      AMERICAN SAMOA|\n|          AR|           ARGENTINA|\n|          AS|           AUSTRALIA|\n|          AT|ASHMORE AND CARTI...|\n|          AU|             AUSTRIA|\n|          AV|            ANGUILLA|\n|          AX|             ANTIGUA|\n|          AY|          ANTARCTICA|\n|          AZ|              AZORES|\n|          BA|             BAHRAIN|\n+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "                .format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(\"stationlist.csv\")\n",
    "stations.createOrReplaceTempView('stations')                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+------------+\n|STN_NO|COUNTRY_ABBR|\n+------+------------+\n|012240|          NO|\n|020690|          SW|\n|020870|          SW|\n|021190|          SW|\n|032690|          UK|\n|033450|          UK|\n|039290|          UK|\n|039790|          EI|\n|040480|          IC|\n|041300|          IC|\n|060100|          FO|\n|061443|          DA|\n|063401|          NL|\n|071910|          FR|\n|092640|          GM|\n|123766|          PL|\n|125990|          PL|\n|129700|          HU|\n|132240|          HR|\n|156500|          BU|\n+------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('STN_NO', 'string'), ('COUNTRY_ABBR', 'string')]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "stations.dtypes"
   ]
  },
  {
   "source": [
    "## 2. Join the stationlist.csv with the countrylist.csv to get the full country name for each station number.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_n_countries = spark.sql(\"\"\"\n",
    "select a.STN_NO\n",
    "     , b.COUNTRY_ABBR\n",
    "     , b.COUNTRY_FULL\n",
    "from stations a inner join countries b on a.COUNTRY_ABBR = b.COUNTRY_ABBR\n",
    "\"\"\")\n",
    "stations_n_countries.createOrReplaceTempView('stations_n_countries') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+------------+--------------+\n|STN_NO|COUNTRY_ABBR|  COUNTRY_FULL|\n+------+------------+--------------+\n|012240|          NO|        NORWAY|\n|020690|          SW|        SWEDEN|\n|020870|          SW|        SWEDEN|\n|021190|          SW|        SWEDEN|\n|032690|          UK|UNITED KINGDOM|\n|033450|          UK|UNITED KINGDOM|\n|039290|          UK|UNITED KINGDOM|\n|039790|          EI|       IRELAND|\n|040480|          IC|       ICELAND|\n|041300|          IC|       ICELAND|\n|060100|          FO| FAROE ISLANDS|\n|061443|          DA|       DENMARK|\n|063401|          NL|   NETHERLANDS|\n|071910|          FR|        FRANCE|\n|092640|          GM|       GERMANY|\n|123766|          PL|        POLAND|\n|125990|          PL|        POLAND|\n|129700|          HU|       HUNGARY|\n|132240|          HR|       CROATIA|\n|156500|          BU|      BULGARIA|\n+------+------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "stations_n_countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "stations.count() - stations_n_countries.count()"
   ]
  },
  {
   "source": [
    "## 3. Join the global weather data with the full country names by station number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "select a.*\n",
    "     , b.COUNTRY_ABBR\n",
    "     , b.COUNTRY_FULL\n",
    "from weather a inner join stations_n_countries b on a.`STN---` = b.STN_NO\n",
    "\"\"\")\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+------------+\n|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD| GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|COUNTRY_ABBR|COUNTRY_FULL|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+------------+\n|010875|99999|20190101|41.1|30.1|9999.9|9999.9|  5.9|46.7| 59.1| 74.0|44.6*|37.4*|99.99|999.9|011010|          NO|      NORWAY|\n|010875|99999|20190102|40.5|29.0|9999.9|9999.9|  6.2|20.5| 32.1| 44.1|41.0*|37.4*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190103|43.0|36.6|9999.9|9999.9|  6.1|13.5| 21.0|999.9|44.6*|41.0*|0.00I|999.9|000000|          NO|      NORWAY|\n|010875|99999|20190104|46.7|44.4|9999.9|9999.9|  5.8|27.4| 33.0| 40.0|48.2*|42.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190105|46.5|44.1|9999.9|9999.9|  6.1|18.3| 25.1|999.9|48.2*|44.6*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190106|45.5|39.4|9999.9|9999.9|  5.7|16.4| 24.1|999.9|46.4*|42.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190107|45.1|40.5|9999.9|9999.9|  5.4|28.5| 47.0| 56.9|48.2*|42.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190108|42.3|33.6|9999.9|9999.9|  6.1|35.7| 44.1| 55.9|46.4*|37.4*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190109|40.7|31.0|9999.9|9999.9|  6.2|16.8| 25.1| 35.0|42.8*|39.2*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190110|47.2|45.3|9999.9|9999.9|  3.5|25.7| 29.9|999.9|48.2*|41.0*|99.99|999.9|110000|          NO|      NORWAY|\n|010875|99999|20190111|44.5|39.6|9999.9|9999.9|  5.8|19.5| 29.9|999.9|46.4*|42.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190112|45.2|41.6|9999.9|9999.9|  5.9|27.3| 35.0| 44.1|46.4*|42.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190113|41.5|34.6|9999.9|9999.9|  5.7|35.3| 48.0| 60.0|46.4*|32.0*|99.99|999.9|111000|          NO|      NORWAY|\n|010875|99999|20190114|34.2|28.0|9999.9|9999.9|  5.5|25.8| 46.0| 60.0|37.4*|30.2*|99.99|999.9|011000|          NO|      NORWAY|\n|010875|99999|20190115|43.3|36.8|9999.9|9999.9|  5.8|25.8| 40.0| 46.0|46.4*|35.6*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190116|41.4|36.3|9999.9|9999.9|  6.1|27.3| 45.1| 48.0|46.4*|33.8*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190117|36.5|28.9|9999.9|9999.9|  5.8|23.5| 42.0| 53.0|39.2*|33.8*|99.99|999.9|011000|          NO|      NORWAY|\n|010875|99999|20190118|39.4|26.7|9999.9|9999.9|  6.1|12.1| 18.1| 21.0|41.0*|32.0*|99.99|999.9|010000|          NO|      NORWAY|\n|010875|99999|20190119|40.9|33.8|9999.9|9999.9|  6.2| 9.9| 15.0|999.9|42.8*|39.2*|0.00I|999.9|000000|          NO|      NORWAY|\n|010875|99999|20190120|42.3|36.5|9999.9|9999.9|  6.2|20.8| 34.0| 36.9|42.8*|39.2*|99.99|999.9|010000|          NO|      NORWAY|\n+------+-----+--------+----+----+------+------+-----+----+-----+-----+-----+-----+-----+-----+------+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15249"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "weather.count() - df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4158416"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('NTEMP', df.TEMP.cast('float')) \\\n",
    "       .withColumn('NWDSP', df.WDSP.cast('float')) \\\n",
    "       .withColumn('tornado', substring(col('FRSHTT'), -1, 1).cast('int')) \\\n",
    "       .withColumn('dt', to_date('YEARMODA', 'yyyyMMdd'))\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('STN---', 'string'),\n",
       " ('WBAN', 'string'),\n",
       " ('YEARMODA', 'string'),\n",
       " ('TEMP', 'string'),\n",
       " ('DEWP', 'string'),\n",
       " ('SLP', 'string'),\n",
       " ('STP', 'string'),\n",
       " ('VISIB', 'string'),\n",
       " ('WDSP', 'string'),\n",
       " ('MXSPD', 'string'),\n",
       " ('GUST', 'string'),\n",
       " ('MAX', 'string'),\n",
       " ('MIN', 'string'),\n",
       " ('PRCP', 'string'),\n",
       " ('SNDP', 'string'),\n",
       " ('FRSHTT', 'string'),\n",
       " ('COUNTRY_ABBR', 'string'),\n",
       " ('COUNTRY_FULL', 'string'),\n",
       " ('NTEMP', 'float'),\n",
       " ('NWDSP', 'float'),\n",
       " ('tornado', 'int'),\n",
       " ('dt', 'date')]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "source": [
    "# Step 2 - Questions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Which country had the hottest average mean temperature over the year?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+------------+-----+-----+\n|STN---|WBAN|YEARMODA|TEMP|DEWP|SLP|STP|VISIB|WDSP|MXSPD|GUST|MAX|MIN|PRCP|SNDP|FRSHTT|COUNTRY_ABBR|COUNTRY_FULL|NTEMP|NWDSP|\n+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+------------+-----+-----+\n+------+----+--------+----+----+---+---+-----+----+-----+----+---+---+----+----+------+------------+------------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from df\n",
    "where NTEMP >= 9999.9\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----------------+\n| country|         avg_temp|\n+--------+-----------------+\n|DJIBOUTI|90.06114474836602|\n+--------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select country\n",
    "     , avg_temp\n",
    "from (\n",
    "    select COUNTRY_FULL as country\n",
    "         , avg(NTEMP) as avg_temp\n",
    "    from df\n",
    "    where NTEMP < 9999.9\n",
    "    group by COUNTRY_FULL\n",
    "    )\n",
    "order by avg_temp desc\n",
    "limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "source": [
    "##  2. Which country had the most consecutive days of tornadoes/funnel cloud formations?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_event = df.withColumn(\"last_event\", lag('dt').over(Window.partitionBy('COUNTRY_FULL').orderBy('dt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_in_day = last_event.withColumn('lag_in_day', (col('dt').astype(\"long\") - col('last_event').astype(\"long\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o409.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 54 in stage 276.0 failed 1 times, most recent failure: Lost task 54.0 in stage 276.0 (TID 7251, 192.168.15.142, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2141/0x0000000840f02040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD$$Lambda$2137/0x0000000840ef1040.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2104/0x0000000840ebe040.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2141/0x0000000840f02040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD$$Lambda$2137/0x0000000840ef1040.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2104/0x0000000840ebe040.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-8f318579cb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlag_in_day\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github.com/PaytmLabs/TheChallenge/venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github.com/PaytmLabs/TheChallenge/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github.com/PaytmLabs/TheChallenge/venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github.com/PaytmLabs/TheChallenge/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github.com/PaytmLabs/TheChallenge/venv/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o409.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 54 in stage 276.0 failed 1 times, most recent failure: Lost task 54.0 in stage 276.0 (TID 7251, 192.168.15.142, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2141/0x0000000840f02040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD$$Lambda$2137/0x0000000840ef1040.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2104/0x0000000840ebe040.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:343)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2141/0x0000000840f02040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD$$Lambda$2137/0x0000000840ef1040.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2104/0x0000000840ebe040.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "lag_in_day.toPandas()"
   ]
  },
  {
   "source": [
    "## 3. Which country had the second highest average mean wind speed over the year?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+--------+-----+-----+------+------+-----+-----+-----+-----+------+------+-----+-----+------+------------+------------+-----+-----+\n|STN---| WBAN|YEARMODA| TEMP| DEWP|   SLP|   STP|VISIB| WDSP|MXSPD| GUST|   MAX|   MIN| PRCP| SNDP|FRSHTT|COUNTRY_ABBR|COUNTRY_FULL|NTEMP|NWDSP|\n+------+-----+--------+-----+-----+------+------+-----+-----+-----+-----+------+------+-----+-----+------+------------+------------+-----+-----+\n|232050|99999|20191223| -0.2| -3.6|1015.5|1014.1|  5.7|999.9|999.9|999.9|  16.0| -7.6*|0.02F|999.9|101000|          RS|      RUSSIA| -0.2|999.9|\n|232050|99999|20191224|-17.3|-22.1|1019.0|1017.4|  4.7|999.9|999.9|999.9| -9.6*|-22.7*|0.01F| 19.7|100000|          RS|      RUSSIA|-17.3|999.9|\n|232050|99999|20191225|-21.8|-27.0|1023.1|1021.6| 12.4|999.9|999.9|999.9| -17.9| -25.6|0.00F| 19.7|000000|          RS|      RUSSIA|-21.8|999.9|\n|255610|99999|20190113|-31.5|-38.2|1021.2|1020.0| 31.1|999.9|999.9|999.9|-28.5*| -35.5|0.00I|  7.9|000000|          RS|      RUSSIA|-31.5|999.9|\n|255610|99999|20190119|-10.9|-17.4|1021.1|1020.0| 12.4|999.9|999.9|999.9| -8.5*|-14.4*|99.99|999.9|001000|          RS|      RUSSIA|-10.9|999.9|\n|255610|99999|20190121| -1.5| -7.6|1025.6|1024.5| 10.9|999.9|999.9|999.9|  0.7*| -4.5*|99.99|  7.9|001000|          RS|      RUSSIA| -1.5|999.9|\n|255610|99999|20190123|-25.9|-32.4|1015.6|1014.5| 31.1|999.9|999.9|999.9|-19.1*| -31.7|0.00I|  7.9|000000|          RS|      RUSSIA|-25.9|999.9|\n|255610|99999|20190130|-24.4|-31.0|1023.1|1022.0| 31.1|999.9|999.9|999.9|-20.7*| -28.5|0.00I|  7.9|000000|          RS|      RUSSIA|-24.4|999.9|\n|255610|99999|20190222|  9.4|  4.0|1002.3|1001.3|  8.1|999.9|999.9|999.9| 14.9*|  1.2*|0.02E|999.9|001000|          RS|      RUSSIA|  9.4|999.9|\n|255610|99999|20190309| -9.6|-16.1|1001.8|1000.8| 21.7|999.9|999.9|999.9|   1.0| -21.3|0.00E| 14.6|000000|          RS|      RUSSIA| -9.6|999.9|\n|296450|99999|20190101| -0.3| -3.5|1037.7|1017.4|  1.9|999.9|999.9|999.9|   9.0|-10.7*|0.02F| 18.9|001000|          RS|      RUSSIA| -0.3|999.9|\n|296450|99999|20190103|-13.3|-18.3|1042.0|1021.0|  3.0|999.9|999.9|999.9|  -4.0|-20.7*|0.00F| 18.9|001000|          RS|      RUSSIA|-13.3|999.9|\n|296450|99999|20190104|-17.9|-23.6|1035.6|1014.5|  0.9|999.9|999.9|999.9|  -9.2|-22.4*|0.01F| 18.9|001000|          RS|      RUSSIA|-17.9|999.9|\n|296450|99999|20190202|-34.7|-43.1|1042.6|1020.5|  4.2|999.9|999.9|999.9| -24.9| -45.0|0.00F| 23.2|000000|          RS|      RUSSIA|-34.7|999.9|\n|296450|99999|20190207|-24.5|-32.9|1048.7|1027.0|  2.6|999.9|999.9|999.9| -11.2| -33.2|0.00F| 22.8|000000|          RS|      RUSSIA|-24.5|999.9|\n|296450|99999|20190208|-24.6|-32.9|1050.5|1028.8|  2.3|999.9|999.9|999.9|  -4.9| -36.2|0.00F| 22.8|000000|          RS|      RUSSIA|-24.6|999.9|\n|296450|99999|20190209|-25.3|-33.7|1043.0|1021.4|  1.2|999.9|999.9|999.9|  -9.9| -38.2|0.00F| 22.4|000000|          RS|      RUSSIA|-25.3|999.9|\n|296450|99999|20190210|-18.7|-27.4|1030.3|1009.2|  0.8|999.9|999.9|999.9|  -4.5| -34.2|0.00F| 22.4|000000|          RS|      RUSSIA|-18.7|999.9|\n|296450|99999|20190211|-10.1|-18.1|1032.2|1011.5|  1.8|999.9|999.9|999.9|   4.3| -26.5|0.00F| 22.4|000000|          RS|      RUSSIA|-10.1|999.9|\n|296450|99999|20190212| -6.4|-12.3|1035.1|1014.6|  6.7|999.9|999.9|999.9|  13.3| -17.5|0.00F| 22.8|001000|          RS|      RUSSIA| -6.4|999.9|\n+------+-----+--------+-----+-----+------+------+-----+-----+-----+-----+------+------+-----+-----+------+------------+------------+-----+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from df\n",
    "where NWDSP >= 999.9\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----------------+\n|country|   avg_wind_speed|\n+-------+-----------------+\n| ZAMBIA|5.920833328117927|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select country\n",
    "     , avg_wind_speed\n",
    "from (\n",
    "        select *\n",
    "            , row_number() over (order by country desc) as rn\n",
    "        from (\n",
    "                select COUNTRY_FULL as country\n",
    "                    , avg(NWDSP) as avg_wind_speed\n",
    "                from df\n",
    "                where NWDSP < 999.9\n",
    "                group by COUNTRY_FULL\n",
    "        )\n",
    ")\n",
    "where rn = 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}